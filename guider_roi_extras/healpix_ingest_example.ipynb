{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal HEALPix-aware Butler ingestion example\n",
    "\n",
    "This standalone example registers a dataset type bound to the correct HEALPix dimension (based on NSIDE) and ingests per-pixel CSV catalogs using that dimension in the dataId. Adjust the paths and names as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated fresh data in: data/Monster_guide\n",
      "✅ Created vignetting file: data/vignetting_vs_angle.npz\n",
      "🆕 Fresh repo will be created at: data/monster_guide_repo_1755196108\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "\n",
    "# Fresh repo and data locations\n",
    "REPO_PATH = f\"data/monster_guide_repo_{int(time.time())}\"\n",
    "COLLECTION = f\"monster_guide_{int(time.time())}\"\n",
    "DATASET_NAME = \"monster_guide_catalog\"\n",
    "CATALOG_PATH = \"data/Monster_guide\"\n",
    "VIGNETTING_FILE = \"data/vignetting_vs_angle.npz\"\n",
    "NSIDE = 32\n",
    "\n",
    "# Create directories\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "Path(CATALOG_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate a simple vignetting file\n",
    "theta = np.linspace(0.0, 2.0, 51)\n",
    "# Near-flat vignetting for simplicity\n",
    "vignetting = 1.0 - 0.0005 * theta**2\n",
    "np.savez(VIGNETTING_FILE, theta=theta, vignetting=vignetting)\n",
    "\n",
    "# Generate a few minimal HEALPix catalog CSVs with required columns\n",
    "# Choose some arbitrary pixel IDs\n",
    "pixel_ids = [10467, 10466, 10586]\n",
    "num_stars = 25\n",
    "for pid in pixel_ids:\n",
    "    # Random coordinates around some RA/Dec\n",
    "    ra = np.deg2rad(120.0 + np.random.uniform(-2.0, 2.0, size=num_stars))\n",
    "    dec = np.deg2rad(-45.0 + np.random.uniform(-2.0, 2.0, size=num_stars))\n",
    "    guide_flag = np.random.randint(0, 128, size=num_stars)\n",
    "    mags = {\n",
    "        \"mag_u\": 15.0 + np.random.uniform(0, 2.0, size=num_stars),\n",
    "        \"mag_g\": 14.0 + np.random.uniform(0, 2.0, size=num_stars),\n",
    "        \"mag_r\": 13.5 + np.random.uniform(0, 2.0, size=num_stars),\n",
    "        \"mag_i\": 13.8 + np.random.uniform(0, 2.0, size=num_stars),\n",
    "        \"mag_z\": 14.2 + np.random.uniform(0, 2.0, size=num_stars),\n",
    "        \"mag_y\": 14.6 + np.random.uniform(0, 2.0, size=num_stars),\n",
    "    }\n",
    "    gaia_G = 14.0 + np.random.uniform(0, 2.0, size=num_stars)\n",
    "    healpix_id = np.full(num_stars, pid)\n",
    "\n",
    "    tbl = Table(\n",
    "        {\n",
    "            \"coord_ra\": ra,\n",
    "            \"coord_dec\": dec,\n",
    "            \"gaia_G\": gaia_G,\n",
    "            **mags,\n",
    "            \"guide_flag\": guide_flag,\n",
    "            \"healpix_id\": healpix_id,\n",
    "        }\n",
    "    )\n",
    "    tbl.write(Path(CATALOG_PATH) / f\"{pid}.csv\", overwrite=True)\n",
    "\n",
    "print(f\"✅ Generated fresh data in: {CATALOG_PATH}\")\n",
    "print(f\"✅ Created vignetting file: {VIGNETTING_FILE}\")\n",
    "print(f\"🆕 Fresh repo will be created at: {REPO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_NAME monster_guide_catalog and existing set()\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "ref not found for dataset monster_guide_catalog and collections = ['monster_guide_1755196108']\n",
      "✅ Ingested monster_guide_catalog into fresh repo: data/monster_guide_repo_1755196108\n",
      "   Collection: monster_guide_1755196108\n",
      "   HEALPix dimension: healpix5\n"
     ]
    }
   ],
   "source": [
    "from lsst.daf.butler import Butler, DatasetType, CollectionType\n",
    "\n",
    "# Create fresh repo and connect writeable\n",
    "Butler.makeRepo(REPO_PATH)\n",
    "butler = Butler(REPO_PATH, writeable=True)\n",
    "\n",
    "# Create a RUN collection explicitly (script parity)\n",
    "try:\n",
    "    butler.registry.registerCollection(COLLECTION, CollectionType.RUN)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Register dataset type bound to healpix dimension using DimensionGroup (script parity)\n",
    "level = int(np.log2(NSIDE))\n",
    "hp_dim = f\"healpix{level}\"\n",
    "hp_group = butler.registry.dimensions.conform([hp_dim])\n",
    "\n",
    "existing = {dt.name for dt in butler.registry.queryDatasetTypes()}\n",
    "print(f\"DATASET_NAME {DATASET_NAME} and existing {existing}\")\n",
    "if DATASET_NAME not in existing:\n",
    "    dt = DatasetType(DATASET_NAME, dimensions=hp_group, storageClass='ArrowAstropy')\n",
    "    butler.registry.registerDatasetType(dt)\n",
    "else:\n",
    "    dt = butler.registry.getDatasetType(DATASET_NAME)\n",
    "\n",
    "# Idempotent ingest into the fresh repo\n",
    "for csv_path in Path(CATALOG_PATH).glob('*.csv'):\n",
    "    try:\n",
    "        pixel_id = int(csv_path.stem)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    dataId = {hp_dim: pixel_id}\n",
    "    ref = butler.registry.findDataset(DATASET_NAME, dataId=dataId, collections=[COLLECTION])\n",
    "    if ref is None:\n",
    "        print(f\"ref not found for dataset {DATASET_NAME} and collections = {[COLLECTION]}\")\n",
    "        tbl = Table.read(csv_path)\n",
    "        ref = butler.put(tbl, dt, dataId=dataId, run=COLLECTION)\n",
    "    else:\n",
    "        print(f\"✅ ref found {ref} for dataset {DATASET_NAME} and collections = {[COLLECTION]}\")\n",
    "        butler.registry.associate(COLLECTION, [ref])\n",
    "\n",
    "print(f\"✅ Ingested {DATASET_NAME} into fresh repo: {REPO_PATH}\")\n",
    "print(f\"   Collection: {COLLECTION}\")\n",
    "print(f\"   HEALPix dimension: {hp_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Repo: data/monster_guide_repo_1755196108\n",
      "📚 Collection (run): monster_guide_1755196108\n",
      "DATASET_NAME is monster_guide_catalog AND existing is {'monster_guide_catalog'}\n",
      "Derived HEALPix: dimension=healpix5 → level=5 → nside=32\n",
      "Derived HEALPix: dimension=healpix5 → level=5 → nside=32\n",
      "Found 14 datasets for 'monster_guide_catalog' in collection 'monster_guide_1755196108'\n",
      "Sample data IDs:\n",
      "  dataId={healpix5: 10467}\n",
      "  dataId={healpix5=10467}}\n",
      "  dataId={healpix5: 10466}\n",
      "  dataId={healpix5=10466}}\n",
      "  dataId={healpix5: 10700}\n",
      "  dataId={healpix5=10700}}\n",
      "  dataId={healpix5: 10701}\n",
      "  dataId={healpix5=10701}}\n",
      "  dataId={healpix5: 10585}\n",
      "  dataId={healpix5=10585}}\n"
     ]
    }
   ],
   "source": [
    "# Verification-only cell for the fresh repo; no ingestion here\n",
    "from lsst.daf.butler import Butler\n",
    "\n",
    "butler = Butler(REPO_PATH)\n",
    "print(f\"📦 Repo: {REPO_PATH}\")\n",
    "print(f\"📚 Collection (run): {COLLECTION}\")\n",
    "\n",
    "# List dataset types\n",
    "existing = {dt.name for dt in butler.registry.queryDatasetTypes()}\n",
    "print(f\"DATASET_NAME is {DATASET_NAME} AND existing is {existing}\")\n",
    "\n",
    "# Derive nside from dataset type's dimensions (not from dataId)\n",
    "dt = butler.registry.getDatasetType(DATASET_NAME)\n",
    "\n",
    "# Prefer iterating the required set directly (healpix{level} is required)\n",
    "dim_names = dt.dimensions.required\n",
    "hp_dims = [name for name in dim_names if str(name).startswith(\"healpix\")]\n",
    "\n",
    "if hp_dims:\n",
    "    hp_name = str(next(iter(hp_dims)))\n",
    "    level = int(hp_name.replace(\"healpix\", \"\"))\n",
    "    derived_nside = 2 ** level\n",
    "    print(f\"Derived HEALPix: dimension={hp_name} → level={level} → nside={derived_nside}\")\n",
    "else:\n",
    "    print(\"Could not locate HEALPix dimension on dataset type.\")\n",
    "\n",
    "\n",
    "# List datasets for this type in the run\n",
    "refs = list(butler.registry.queryDatasets(DATASET_NAME, collections=[COLLECTION]))\n",
    "print(f\"Found {len(refs)} datasets for '{DATASET_NAME}' in collection '{COLLECTION}'\")\n",
    "if refs:\n",
    "    sample = refs[:5]\n",
    "    print(\"Sample data IDs:\")\n",
    "    for r in sample:\n",
    "        print(f\"  dataId={r.dataId.required}\")\n",
    "        print(f\"  dataId={{\" + \", \".join(f\"{k}={v}\" for k, v in r.dataId.required.items()) + \"}}\")\n",
    "        #print(f\"  dataId={dict(r.dataId)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEALPix-aware Butler ingestion: Dimensions and Data IDs\n",
    "\n",
    "This note summarizes how to register and ingest HEALPix-partitioned datasets with Butler, and clarifies how Dimensions and Data IDs work.\n",
    "\n",
    "## Dimensions (what they are and are not)\n",
    "- Dimensions are not arbitrary metadata. They are defined by the repository’s DimensionUniverse (schema) and must be one of the known names.\n",
    "- For HEALPix, the dimension name encodes the level: level = log2(nside). Examples:\n",
    "  - nside = 32 → level 5 → dimension name: `healpix5`\n",
    "  - nside = 64 → level 6 → dimension name: `healpix6`\n",
    "- You cannot add free-form keys (e.g., `nside`) to a DatasetType’s dimensions. If you need to carry `nside`, put it in the dataset payload (metadata/columns) or encode it in the dataset type or collection name.\n",
    "\n",
    "## Registering DatasetTypes (HEALPix and dimensionless)\n",
    "Prefer building a DimensionGroup from canonical names, then use it when creating the DatasetType.\n",
    "\n",
    "```python\n",
    "# Given a writeable Butler\n",
    "from lsst.daf.butler import DatasetType, CollectionType\n",
    "\n",
    "# Create a RUN collection explicitly\n",
    "butler.registry.registerCollection(COLLECTION, CollectionType.RUN)\n",
    "\n",
    "# Build the HEALPix dimension group from names (recommended)\n",
    "level = int(np.log2(NSIDE))\n",
    "hp_dim = f\"healpix{level}\"\n",
    "hp_group = butler.registry.dimensions.conform([hp_dim])\n",
    "\n",
    "# Register the catalog dataset type (HEALPix-bound)\n",
    "dt = DatasetType(\n",
    "    \"monster_guide_catalog\",\n",
    "    dimensions=hp_group,\n",
    "    storageClass=\"ArrowAstropy\",  # or another appropriate storage class\n",
    ")\n",
    "butler.registry.registerDatasetType(dt)\n",
    "\n",
    "# Register a dimensionless dataset type (e.g., vignetting)\n",
    "vt = DatasetType(\n",
    "    \"vignetting_correction\",\n",
    "    dimensions=butler.registry.dimensions.empty,\n",
    "    storageClass=\"StructuredDataDict\",\n",
    ")\n",
    "butler.registry.registerDatasetType(vt)\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- Passing a set of names (e.g., `{hp_dim}`) is also supported in some versions, but then you must also pass `universe=butler.registry.dimensions`. Using `dimensions.conform([...])` produces a DimensionGroup and avoids that requirement.\n",
    "\n",
    "## Data IDs (how to address a dataset)\n",
    "- Butler requires a mapping from required dimension name to value. For HEALPix:\n",
    "  - `dataId = {hp_dim: pixel_id}` where `hp_dim == 'healpix5'` (for nside=32)\n",
    "- Alternative calling style (kwargs):\n",
    "  - `butler.put(obj, dt, run=COLLECTION, **{hp_dim: pixel_id})`\n",
    "  - If you know `hp_dim` concretely: `butler.put(obj, dt, run=COLLECTION, healpix5=pixel_id)`\n",
    "- Dimensionless datasets use an empty mapping: `dataId = {}`\n",
    "\n",
    "## Idempotent ingestion pattern\n",
    "Avoid uniqueness conflicts by checking whether a dataset already exists for the (datasetType, dataId) in your run collection:\n",
    "\n",
    "```python\n",
    "from astropy.table import Table\n",
    "\n",
    "for csv_path in Path(CATALOG_PATH).glob(\"*.csv\"):\n",
    "    pixel_id = int(csv_path.stem)  # filename like 10467.csv\n",
    "    dataId = {hp_dim: pixel_id}\n",
    "\n",
    "    # Check in target collection (run)\n",
    "    ref = butler.registry.findDataset(\"monster_guide_catalog\", dataId=dataId, collections=[COLLECTION])\n",
    "    if ref is None:\n",
    "        table = Table.read(csv_path)\n",
    "        butler.put(table, dt, dataId=dataId, run=COLLECTION)\n",
    "    else:\n",
    "        # ensure association to the run (no-op if already associated)\n",
    "        butler.registry.associate(COLLECTION, [ref])\n",
    "```\n",
    "\n",
    "## Common pitfalls and fixes\n",
    "- “Butler is read-only.” → open with `Butler(REPO_PATH, writeable=True)` and create a run (collection) for writes.\n",
    "- “NoDefaultCollectionError.” → pass the collection explicitly: `collections=[COLLECTION]` to queries like `findDataset`.\n",
    "- “UNIQUE constraint failed … dataset_tags …” → you are inserting a duplicate (same datasetType and dataId) into the same run. Use the idempotent pattern above, or start with a fresh repo/run.\n",
    "- “If dimensions is not a DimensionGroup, a universe must be provided.” → either pass `universe=butler.registry.dimensions` or build a DimensionGroup with `dimensions.conform([...])`.\n",
    "\n",
    "## Where this is used here\n",
    "- Example notebook: `guider_roi_extra_other_branch/healpix_ingest_example.ipynb` shows data generation, repo creation, HEALPix-aware registration, idempotent ingest, and verification.\n",
    "- Ingestion script: `guider_roi_extra_other_branch/ingest_guider_data.py` uses the same DimensionGroup approach and also ingests a dimensionless vignetting dataset.\n",
    "\n",
    "## Quick reference\n",
    "- Dimension name for HEALPix: `healpix{level}` with `level = int(log2(nside))`\n",
    "- DatasetType for HEALPix: `DatasetType(name, dimensions=butler.registry.dimensions.conform([healpix{level}]), storageClass=...)`\n",
    "- DatasetType for dimensionless data: `DatasetType(name, dimensions=butler.registry.dimensions.empty, storageClass=...)`\n",
    "- Data ID for HEALPix: `{healpix{level}: pixel_id}` (mapping), or kwargs equivalent\n",
    "- Run/collection for writes: `registerCollection(COLLECTION, CollectionType.RUN)` and pass `run=COLLECTION` when calling `put`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
